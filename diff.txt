+import ollama
+import streamlit as st
+from icon import page_icon
+from openai import OpenAI
+
+from utilities.utils import process_pdf, chat_stream,chat_pdf
+Please generate a concise, one-line commit message for these changes
+st.set_page_config(
+    page_title="Chat playground",
+    page_icon="ðŸ’¬",
+    layout="wide",
+    initial_sidebar_state="expanded",
+)
+
+def extract_model_names(models_info: list) -> tuple:
+    """
+    Extracts the model names from the models information.
+
+    :param models_info: A dictionary containing the models' information.
+
+    Return:
+        A tuple containing the model names.
+    """
+
+    return tuple(model["name"] for model in models_info["models"])
+
+def pdf_toggle_fun():
+   
+    if st.session_state.pdf_toggle:
+        st.session_state.pdf_processed = False
+        st.session_state.vector_db = None
+        
+     
+def main():
+    
+    """
+    The main function that runs the application.
+    """
+    
+    st.markdown("""
+        <style>
+            .header {
+                font-size: 36px;
+                font-weight: bold;
+                color: #1e90ff;
+                text-align: center;
+                margin-top: 20px;
+            }
+            .subheader {
+                font-size: 18px;
+                color: #4b4b4b;
+                text-align: center;
+                margin-top: 10px;
+                font-style: italic;
+            }
+        </style>
+        <div class="header">ðŸ’¬ Chat and Question by Uploading PDF ðŸ“„</div>
+        <div class="subheader">Ask questions based on the content of your PDF document</div>
+    """, unsafe_allow_html=True)
+
+    page_icon("ðŸ’¬")
+    st.subheader("Ollama Playground", divider="red", anchor=False)
+
+    client = OpenAI(
+        base_url="http://localhost:11434/v1",
+        api_key="ollama",  # required, but unused
+    )
+    
+    models_info = ollama.list()
+    
+    available_models = extract_model_names(models_info)
+    
+    if available_models:
+        selected_model = st.selectbox(
+            "Pick a model available locally on your system â†“", available_models
+        )
+    else:
+        st.warning("You have not pulled any model from Ollama yet!", icon="âš ï¸")
+        if st.button("Go to settings to download a model"):
+            st.page_switch("pages/03_âš™ï¸_Settings.py")
+            
+            
+    # Streaming mode toggle
+    st.toggle("Enable PDF Question-Answer Mode",key='pdf_toggle' , on_change=pdf_toggle_fun)
+     
+    message_container = st.container(height=500, border=True)
+    
+    if "messages" not in st.session_state:
+        st.session_state.messages = []
+        
+    if "vector_db" not in st.session_state:
+         st.session_state.vector_db = None
+    if "pdf_processed" not in st.session_state:
+        st.session_state.pdf_processed = False
+        
+   
+    
+    #Displaying Previous Messages:
+    for message in st.session_state.messages: 
+        avatar = "ðŸ¤–" if message["role"] == "assistant" else "ðŸ˜Ž"
+        with message_container.chat_message(message["role"], avatar=avatar):
+            st.markdown(message["content"])
+         
+    
+        
+    # Input area for prompt or file upload
+    col1, col2 = st.columns([3, 1])
+   
+    
+    if st.session_state.pdf_toggle:
+        with col1:
+            prompt = st.chat_input("Enter a prompt here...")
+        with col2:
+            uploaded_file = st.file_uploader("Upload a PDF", type="pdf") 
+            
+         # Process PDF file if uploaded and not yet processed
+        if uploaded_file and not st.session_state.pdf_processed:
+            st.session_state.vector_db = process_pdf(uploaded_file,message_container)
+            st.session_state.pdf_processed = True
+            message_container.success("PDF processed and vector database created successfully.")
+
+            with message_container.chat_message("assistant", avatar="ðŸ¤–"):
+                 st.markdown("The PDF has been processed. You can now ask questions based on its content.")
+        
+    
+        # Handle prompt input with retrieval and RAG processing
+        if prompt and st.session_state.vector_db is not None and uploaded_file:
+                chat_pdf(prompt,message_container,selected_model)
+   
+   
+    else:
+        prompt = st.chat_input("Enter a prompt here...")
+        if prompt:
+            chat_stream(prompt,client,selected_model,message_container)
+        
+            
+   
+                
+   
+            
+    # st.write(st.session_state)
+    
+    
+        
+            
+   
+         
+    
+            
+if __name__ == "__main__":
+    main()
+    
+    
\ No newline at end of file
diff --git "a/02_\360\237\214\213_Multimodal.py" "b/02_\360\237\214\213_Multimodal.py"
new file mode 100644
index 0000000..4b8e31e
--- /dev/null
+++ "b/02_\360\237\214\213_Multimodal.py"
@@ -0,0 +1,42 @@
+import streamlit as st
+import requests
+import base64
+from PIL import Image
+from io import BytesIO
+import json
+import ollama
+from icon import page_icon
+
+
+st.set_page_config(
+    page_title="LLaVA Playground",
+    page_icon="ðŸŒ‹",
+    layout="wide",
+    initial_sidebar_state="expanded",
+)
+
+
+def img_to_base64(image):
+    """
+    Convert an image to base64 format.
+
+    Args:
+        image: PIL.Image - The image to be converted.
+    Returns:
+        str: The base64 encoded image.
+    """
+    buffered = BytesIO()
+    image.save(buffered, format="PNG")
+    return base64.b64encode(buffered.getvalue()).decode()
+
+
+def get_allowed_model_names(models_info: dict) -> tuple:
+    """
+    Returns a tuple containing the names of the allowed models.
+    """
+    allowed_models = ["bakllava:latest", "llava:latest"]
+    return tuple(
+        model
+        for model in allowed_models
+        if model in [m["name"] for m in models_info["models"]]
+    )
\ No newline at end of file
diff --git a/__pycache__/icon.cpython-311.pyc b/__pycache__/icon.cpython-311.pyc
new file mode 100644
index 0000000..a8091aa
Binary files /dev/null and b/__pycache__/icon.cpython-311.pyc differ
diff --git a/icon.py b/icon.py
new file mode 100644
index 0000000..6837dca
--- /dev/null
+++ b/icon.py
@@ -0,0 +1,17 @@
+import streamlit as st
+
+
+def page_icon(emoji: str):
+    """
+    Shows an emoji as a Notion-style page icon.
+
+    :param emoji: The emoji to display.
+
+    Returns:
+        None
+    """
+
+    st.write(
+        f'<span style="font-size: 78px; line-height: 1">{emoji}</span>',
+                                              5
+
diff --git a/utilities/__init__.py b/utilities/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/utilities/__pycache__/__init__.cpython-311.pyc b/utilities/__pycache__/__init__.cpython-311.pyc
new file mode 100644
index 0000000..cc6c3e8
Binary files /dev/null and b/utilities/__pycache__/__init__.cpython-311.pyc differ
diff --git a/utilities/__pycache__/utils.cpython-311.pyc b/utilities/__pycache__/utils.cpython-311.pyc
new file mode 100644
index 0000000..daedddc
Binary files /dev/null and b/utilities/__pycache__/utils.cpython-311.pyc differ
diff --git a/utilities/utils.py b/utilities/utils.py
new file mode 100644
index 0000000..c173b0b
--- /dev/null